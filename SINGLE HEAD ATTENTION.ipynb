{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "  import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# New Input: batch_size=1, seq_len=3, embedding_dim=4\n",
        "# Simulating embeddings for tokens in a sentence about LLMs\n",
        "X = torch.tensor([\n",
        "    [\n",
        "        [1.0, 0.5, 1.5, 0.0],   # token 1\n",
        "        [0.0, 1.5, 0.5, 1.0],   # token 2\n",
        "        [1.0, 1.0, 1.0, 1.0]    # token 3\n",
        "    ]\n",
        "])\n",
        "\n",
        "# Initialize weight matrices for Q, K, V\n",
        "d_model = X.size(-1)\n",
        "W_Q = torch.rand(d_model, d_model)\n",
        "W_K = torch.rand(d_model, d_model)\n",
        "W_V = torch.rand(d_model, d_model)\n",
        "# Compute Query, Key, Value\n",
        "Q = torch.matmul(X, W_Q)\n",
        "K = torch.matmul(X, W_K)\n",
        "V = torch.matmul(X, W_V)\n",
        "\n",
        "# Scaled Dot-Product Attention\n",
        "scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model)\n",
        "attention_weights = F.softmax(scores, dim=-1)\n",
        "context_vector = torch.matmul(attention_weights, V)\n",
        "\n",
        "# Outputs\n",
        "print(\"Input X shape:\", X.shape)\n",
        "print(\"\\nQuery (Q):\\n\", Q)\n",
        "print(\"\\nKey (K):\\n\", K)\n",
        "print(\"\\nValue (V):\\n\", V)\n",
        "print(\"\\nAttention Scores:\\n\", scores)\n",
        "print(\"\\nAttention Weights:\\n\", attention_weights)\n",
        "print(\"\\nContext Vector (Final Output):\\n\", context_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtfYgWZ-P86l",
        "outputId": "ca89833b-e6d8-4d26-c8cc-c8914523aebf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input X shape: torch.Size([1, 3, 4])\n",
            "\n",
            "Query (Q):\n",
            " tensor([[[2.4887, 1.4152, 1.9130, 2.2465],\n",
            "         [1.9255, 1.5357, 1.5933, 1.9167],\n",
            "         [3.0829, 2.2168, 2.3151, 2.7759]]])\n",
            "\n",
            "Key (K):\n",
            " tensor([[[1.1782, 1.1988, 0.9533, 1.3423],\n",
            "         [1.0043, 0.8030, 1.5763, 1.4225],\n",
            "         [1.8076, 1.2909, 1.8739, 1.7338]]])\n",
            "\n",
            "Value (V):\n",
            " tensor([[[2.1265, 1.5818, 1.6806, 2.3997],\n",
            "         [1.9731, 1.8196, 1.9758, 1.5761],\n",
            "         [2.8873, 2.1330, 2.3914, 2.6872]]])\n",
            "\n",
            "Attention Scores:\n",
            " tensor([[[4.7340, 4.9235, 6.9025],\n",
            "         [4.1006, 4.2023, 5.8857],\n",
            "         [6.1115, 6.2371, 8.7927]]])\n",
            "\n",
            "Attention Weights:\n",
            " tensor([[[0.0913, 0.1103, 0.7984],\n",
            "         [0.1240, 0.1372, 0.7388],\n",
            "         [0.0598, 0.0677, 0.8725]]])\n",
            "\n",
            "Context Vector (Final Output):\n",
            " tensor([[[2.7170, 2.0481, 2.2806, 2.5383],\n",
            "         [2.6675, 2.0217, 2.2462, 2.4991],\n",
            "         [2.7799, 2.0788, 2.3207, 2.5947]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BUPJ_WFnR5h2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}