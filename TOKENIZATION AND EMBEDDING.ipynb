{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpizbYsoyj5o",
        "outputId": "659b91cb-5b60-4dac-cd7c-4a7543c6657f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Embeddings:\n",
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
            "\n",
            "Embedding for Token at Index 3:\n",
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Create an embedding layer with 6 tokens and 3 dimensions\n",
        "embedding_layer = torch.nn.Embedding(6, 3)\n",
        "\n",
        "# Display the initial weights (embeddings)\n",
        "print(\"Initial Embeddings:\")\n",
        "print(embedding_layer.weight)\n",
        "\n",
        "# Retrieve the embedding for the token at index 3\n",
        "token_index = torch.tensor([3])\n",
        "token_embedding = embedding_layer(token_index)\n",
        "\n",
        "print(\"\\nEmbedding for Token at Index 3:\")\n",
        "print(token_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Input sentence\n",
        "# -----------------------------\n",
        "sentence = \"I love LLM very much\"\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Tokenization\n",
        "# -----------------------------\n",
        "tokens = sentence.lower().split()\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Vocabulary & token IDs\n",
        "# -----------------------------\n",
        "vocab = {word: idx for idx, word in enumerate(set(tokens))}\n",
        "token_ids = torch.tensor([vocab[token] for token in tokens])\n",
        "\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Token IDs:\", token_ids.tolist())\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Token Embedding\n",
        "# -----------------------------\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 8\n",
        "\n",
        "token_embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "token_embeddings = token_embedding_layer(token_ids)\n",
        "\n",
        "print(\"\\nToken Embeddings:\")\n",
        "print(token_embeddings)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Positional Embedding\n",
        "# -----------------------------\n",
        "sequence_length = len(tokens)\n",
        "position_ids = torch.arange(sequence_length)\n",
        "\n",
        "position_embedding_layer = nn.Embedding(sequence_length, embedding_dim)\n",
        "positional_embeddings = position_embedding_layer(position_ids)\n",
        "\n",
        "print(\"\\nPositional Embeddings:\")\n",
        "print(positional_embeddings)\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Final Embedding (Token + Positional)\n",
        "# -----------------------------\n",
        "final_embeddings = token_embeddings + positional_embeddings\n",
        "\n",
        "print(\"\\nFinal Embeddings (Token + Positional):\")\n",
        "print(final_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qHjFTpjzWOx",
        "outputId": "92ef159e-e073-45ae-aea1-97958d1d197f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['i', 'love', 'nlp', 'very', 'much']\n",
            "Vocabulary: {'very': 0, 'nlp': 1, 'i': 2, 'much': 3, 'love': 4}\n",
            "Token IDs: [2, 4, 1, 0, 3]\n",
            "\n",
            "Token Embeddings:\n",
            "tensor([[-0.4213,  0.8879, -0.3825,  0.0672, -0.6540, -0.4250,  0.1616, -1.1260],\n",
            "        [-0.1535, -1.1988, -2.1718, -1.7100,  0.2413, -0.7121, -0.9829,  0.7354],\n",
            "        [-0.7540,  1.0738, -0.6731,  0.2997, -0.8631,  0.7167, -0.2071,  0.3427],\n",
            "        [-1.4548, -2.1018,  0.3476,  2.4807, -0.5868, -1.0868, -1.1194,  1.1385],\n",
            "        [ 0.5680, -0.4415, -0.1969,  2.0947,  1.3814,  0.3591,  0.5690,  0.3081]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "Positional Embeddings:\n",
            "tensor([[ 1.0493,  0.8459, -0.7381,  0.5146, -1.5782,  0.9765,  0.6253, -1.0222],\n",
            "        [-0.4951, -2.1193, -0.3902, -1.6629, -1.0127, -0.0747, -1.5313, -0.4070],\n",
            "        [ 0.5939,  1.0732, -0.2144,  0.0911,  0.6795,  0.6325,  1.6750,  0.4287],\n",
            "        [ 0.1592, -0.3631, -0.2558, -3.0047,  0.7309, -2.6489, -1.1460,  0.9909],\n",
            "        [ 0.9774,  0.5857, -0.2064, -0.4909,  1.1247,  0.6374, -1.3792,  0.5072]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "Final Embeddings (Token + Positional):\n",
            "tensor([[ 0.6280,  1.7338, -1.1206,  0.5817, -2.2322,  0.5515,  0.7869, -2.1482],\n",
            "        [-0.6486, -3.3181, -2.5620, -3.3729, -0.7714, -0.7868, -2.5142,  0.3284],\n",
            "        [-0.1601,  2.1469, -0.8875,  0.3908, -0.1836,  1.3492,  1.4680,  0.7714],\n",
            "        [-1.2957, -2.4650,  0.0918, -0.5239,  0.1441, -3.7358, -2.2654,  2.1293],\n",
            "        [ 1.5454,  0.1442, -0.4034,  1.6038,  2.5061,  0.9965, -0.8102,  0.8152]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OFJQIsK-1Cok"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}